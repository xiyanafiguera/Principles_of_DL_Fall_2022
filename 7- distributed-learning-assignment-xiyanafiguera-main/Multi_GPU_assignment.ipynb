{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UNIST-LIM-Lab-course/distributed-learning-assignment-xiyanafiguera/blob/main/Multi_GPU_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distributed Learning Assignment\n",
        "\n",
        "--------\n",
        "\n",
        "- This assignment is due on **Nov. 20 (11:59 PM)** - LATE SUBMISSION WILL NOT BE ACCEPTED\n",
        "- Please **carefully follow the instructions** for each assignment.\n",
        "\n",
        "### Grading criteria\n",
        "1. All the codes in this notebook should be runnable.\n",
        "2. Each answer variable is 0.5 point; total 3 points available. "
      ],
      "metadata": {
        "id": "q4kCX2R27SPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Data Parallel / Distributed Data Parallel"
      ],
      "metadata": {
        "id": "Ywo3TwFi73ds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Please answer the following questions:\n",
        "\n",
        "    Q1. [True or False] Data Parallel is multi-threading and Distributed Data Parallel is multi-processing that both supports multi-server training.\n",
        "\n",
        "    Q2. [True or False] Data Parallel has a memory allocation issue while Distributed Data Parallel does not. \n",
        "\n",
        "    Q3. [True or False] In Data Parallel, each GPU computes loss independently after forward pass, and all the gradients are computed during backprop that are shared with other GPUs to update models. \n",
        "\n",
        "    Q4. We would like to use DataParallel for training a model using first, thrid, and fifth GPU on the server (out of 8 GPUs). What is the correct argument for applying Data Parallel on the code below? Note that the order of GPU starts from 0.\n",
        "\n",
        "    ```\n",
        "    model = torch.nn.DataParallel(model, device_ids=Q4_ANSWER)\n",
        "    ```\n",
        "\n",
        "    Q5. Suppose we are using distributed training using 6 GPUs on a single server, where the main file name is `main.py`, and the latest PyTorch version is downloaded. We would like to use TorchElastic. What is the correct argument for distributed training below? \n",
        "\n",
        "    ```\n",
        "    $ torchrun --Q5_1_ANSWER --Q5_2ANSWER main.py\n",
        "    ```\n"
      ],
      "metadata": {
        "id": "PkdRyYuNFWmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Please put the correct answer for each variable.\n",
        "Q1_ANSWER = False\n",
        "\n",
        "Q2_ANSWER = True\n",
        "\n",
        "Q3_ANSWER = False\n",
        "\n",
        "Q4_ANSWER = [0,2,4]\n",
        "\n",
        "Q5_1_ANSWER = 'standalone'\n",
        "Q5_2_ANSWER = 'nproc_per_node=6'"
      ],
      "metadata": {
        "id": "cGxJJVthFXwi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing: DO NOT EDIT THE CELLS BELOW AND DO NOT ADD CELLS BELOW"
      ],
      "metadata": {
        "id": "OFL-6-at_U86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DP/DDP"
      ],
      "metadata": {
        "id": "CfN8P9fbPnJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(Q1_ANSWER)\n",
        "print(Q2_ANSWER)\n",
        "print(Q3_ANSWER)\n",
        "print(Q4_ANSWER)\n",
        "print(Q5_1_ANSWER)\n",
        "print(Q5_2_ANSWER)"
      ],
      "metadata": {
        "id": "RISF2aiBX-zV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddc98498-60fd-4c9b-c243-67930df75c6a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "True\n",
            "False\n",
            "[0, 2, 4]\n",
            "standalone\n",
            "nproc_per_node=6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# THIS CELL IS INTENTIONALLY LEFT AS BLANK."
      ],
      "metadata": {
        "id": "NLfxyHOtPxz2"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}